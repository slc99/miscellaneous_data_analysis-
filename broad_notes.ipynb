{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General pandas commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrane(object, columns=['column1', 'column2']) # converts object into df\n",
    "df = pd.read_csv('path', index_col='column1', parse_dates=['column2']) # read from csv, with 'column1' read as index and 'colum2' read as dates\n",
    "df.dtypes # return data type of each column\n",
    "df.head() # returns first five rows\n",
    "df.tail() # return last five\n",
    "df.describe() # shows basic statistics for numeric columns\n",
    "df.info() # returns info on dtype and nulls\n",
    "df['column1'] # returns a panda series of that column with index and data\n",
    "df.loc[index] # return a row by index\n",
    "df = df.set_index('column_name') # change index to a specific column\n",
    "df.columns # display column names\n",
    "df = df[['column1', 'column3','column12']] # creates a new df with selected columns\n",
    "df.shape # returns (rows, columns)\n",
    "df['column2'] > 1_000 # returns a panda series of booleans depending on if the condition is met for that index\n",
    "subset = df.loc[df['column2'] > 1_000] # use a boolean mask to only select rows where this condition is met\n",
    "subset = df.query('column2 > 1000') # query method does the same as above\n",
    "df['column_name'].isna() # returns pandas series of booleans of whether that column has a na value\n",
    "df = df.loc[~df['column_name'].isna()] # removes rows where column_name is na. The ~ == not, so applying a boolean mask\n",
    "df['column_name'].astype('int') # returns the column cast as an int\n",
    "df['column_name'] = df['column_name'].astype('int') # replaces the column in the original dataframe\n",
    "df['column_name'] = pd.to_datetime(df['column_name']) # returns the column cast as datetime, and replaces it\n",
    "df['column_name'] = pd.to_numeric(df['column_name']) # if column is type string and you want it float or int, let pandas convert it \n",
    "df['ratio'] = df['column_1'] / df['column_2'] # the right hand side returns a pandas series with the divided values attached with the pandas series index\n",
    "df_merged = pd.concat(['df1','df2']) # returns the combined dataframe, think SQL UNION\n",
    "df['column'].plot(kind='hist', bins=50, title='title', figsize=(x_int,y_int)) # creates a histogram of the data. Called on a single column\n",
    "df.plot(kind='scatter', x='column1', y='column2', title='title') # create a scatterplot of the data. Notice it is called on dataframe\n",
    "df.to_csv('output.csv', index=False) # saves the dataframe. Use index = False if the index column is not necessary for understanding the data\n",
    "df['column'] = df['column'].map({'Yes':True, 'No':False}) # the .map() method replaces existing values by what is put in the key dictionary\n",
    "df['column_uppercase'] = df['column'].str.upper() # string method that returns a series with the string values upper case\n",
    "df['stripped'] = df['column'].str.strip() # removes leading and trailing spaces\n",
    "df = df.rename(columns={'old_name':'new_name'}) # renames the columns listed\n",
    "df.groupby('column')['column2'].min() # first groups rows based on column values. Any aggregation functions done with work by each group\n",
    "df['column'].pct_change() # returns the percent change by for a column as a pandas series\n",
    "df['column'].diff() # calculates the difference between rows\n",
    "df_merged = df1.merge(df2, on=['column'], suffixes=('_1','_2')) # merges df1 and df2. Similar to sql join. Suffixes will be affixed to columns not used on the merge\n",
    "df.duplicated(subset=['column']) # returns boolean on if the row is the 2+ copy of that row. Subset is an optional argument to limit considered columns for duplicates\n",
    "df.loc[df.duplicated()] # returns the rows that are duplicated\n",
    "df = df.loc[~df.duplicated()].reset_index(drop=True).copy() # returns rows are not duplicates. Because you are dropping rows, you should reset index and drop the old one\n",
    "df = df.reset_index(drop=True) # resets index and drops the existing index\n",
    "df['column'].value_counts() # prints count for each value - returns a pandas series\n",
    "df_corr = df.corr() # finds the correlation coefficient between each column - might be smart to run on a subset of the data\n",
    "df.groupby('column1')['column2'].agg(['mean','count']) # this will first groupby column1, select column2, and then aggragate over the groups to display mean and count\n",
    "df.sort_values(by=['column1', 'column2'], ascending=False) # sorts the dataframe by the selected columns - first by col1, then with col2. Ascending is optional \n",
    "df['column'].tolist() # returns a list object from a pandas series\n",
    "df = df.assign(new_column_name=df['existing_column'] * 100) # another way to create a new column based on existing columns. Useful for best practice of chaining commands\n",
    "df_numerics = df.select_dtypes(include=[np.number]) # select only numeric columns. Returns a dataframe\n",
    "df.drop_duplicates() # removes duplicated rows\n",
    "df['column'].str.extract('(\\d+)') # extracts the numbers from a sting\n",
    "df['Lightweight'] = ((df['Weight'] < 72.5) & (df['Sex'] == 'M')) # boolean operations in pandas must use & | and be litered in ()\n",
    "df['column'] = df['boolean_column'].astype(int) # will convert a boolean column to 1's and 0's\n",
    "df['column'].isin(example_list) # checks to see if the value is in the example list\n",
    "df['column'].nunique() # prints number of unique values in the column\n",
    "df['year'] = df['datetime'].dt.year # takes the datetime column 'datetime', splits off year, and adds it seperately\n",
    "df.join(other_df[['exchange_for_stake']]) # adds a column along the index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General pandas notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_x = 1980\n",
    "var_y = 20\n",
    "df = df.query('column > @var_x and column2 == @var_y') # you can access variables from the query method with the @ symbol\n",
    "# overwrite the df instead of using inplace\n",
    "# SettingWithCopyWarning: when you select a subset of a df, you should use the .copy() method if you want to create a new df from the slice\n",
    "df = df.query('condition').copy() # will fix the error\n",
    "# use functions to instead of repeating the same data transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting and Seaborn and MatPlotLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme() # sets default theme. This applies to all matplotlib plots\n",
    "sns.heatmap(df_corr, annot=True) # prints out a heatmap of correlation coefficients of columns from previously generated matrix\n",
    "plt.style.use('ggplot') # changes the style of seaborn and matplotlib for the entire document. Good options include 'fivethirtyeight', 'bmh', and 'dark_background'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(x, y)) # changes figsizes for matplotlib and seaborn\n",
    "fig = sns.scatterplot(df, x='column1', y='column2', hue='column3').set(title='Title', xlabel='xlab') # creates a seaborn scatter. Notice the title is set with 'set'\n",
    "# can change seaborn plot size with 'height' and 'aspect' parameters\n",
    "# set index as date column to make the df.plot() method pretty\n",
    "sns.color_palette() # will return the color palette as an object\n",
    "df['column'].plot(type='barh', figsize=(x,y)) # creates a horizontal bar chart of the series. Built from matplotlib0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLearn Commands & Other Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.set_config(transform_output='pandas') # transformer outputs will be in the pandas dataframe they were inputed with, no wrapping required\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42) # splits df in two along test size split\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median') # creates imputer to fill na values with\n",
    "df_nums = df.select_dtypes(include=[np.number]) \n",
    "imputer.fit(df_nums) # fits the imputer with data\n",
    "x = imputer.transform(housing_num) # fills na values with the median that was previously trained. Returns a numpy array\n",
    "imputer.feature_names_in_ # displays the names of the features inputed\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder() # create the ordinal encoder class\n",
    "df_categorical_encoded = ordinal_encoder.fit_transform(df[['column']]) # fit ordinal encoder on 'column' and save the new data\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy='median'), StandardScaler()) # pipelines allow for multiple transformations in a row\n",
    "num_pipeline.steps # attribute with name, process of the preprocessing\n",
    "num_pipeline[1] # returns that step of processing\n",
    "num_pipeline.set_params(simpleimputed__strategy='median') # change the params of pipeline steps with __ to access different levels of nesting\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "num_attribues = ['column1', 'column2', 'column3'] # list of the names of numeric columns\n",
    "cat_attributes = ['column4'] # list of categorical column names\n",
    "preprocessing = ColumnTransformer(\n",
    "    [\n",
    "        ('num', num_pipeline, num_attributes),\n",
    "        ('cat', cat_pipeline, cat_attributes)\n",
    "    ], \n",
    "    remainder='passthrough'\n",
    ") # ColumnTransformer allows us to seperate out which columns go into which pipelines. 'num' and 'cat' are the pathway nicknames, num_pipeline is the actual piepline,\n",
    "# and num_attributes are the column names of numeric columns. The remainder specifies what happens to unspecified columns. drop: dropped, passthrough: unchanged\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logit_clf = LogisticRegression(random_state=42) # create the regression model. Provide hyperparams here\n",
    "logit_clf.fit(X_train, Y_train) # fits model to the provided data\n",
    "\n",
    "# Validating the model -- Classifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "Y_train_prediction = cross_val_predict(model, X_train, Y_train, cv=3) # runs a cross validation with three folds. Returns the list of predicted values for Y as an array\n",
    "from sklearn.metrics import confusion_matrix \n",
    "cm = confusion_matrix(Y_train_predictions, Y_train) # 2x2 matrix, rows = actual class, columns = predicted class. top left is true negatives, bottom left is true positives\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "ConfusionMatrixDisplay.from_predictions(train_labels, labels_logit_prediction) # can add the params normalize='true', value_format=\".0%\" to make it prettier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, roc_auc_score\n",
    "precision_score(Y_train, Y_train_prediction) # percision measures predicted true positive / (all predicted positives), i.e. how trustable are your predictions\n",
    "recall_score(Y_train, Y_train_prediction) # recall measures predicted true positives / (all true positives), i.e. what portion of true positives you catch\n",
    "f1_score(Y_train, Y_train_prediction) # combines the two. Lowest when percision and recall are even\n",
    "fpr, tpr, thresholds = roc_curve(Y_train, Y_train_prediction) # plots true positive rate against false postive rate. \n",
    "plt.plot(fpr, tpr, label='ROC Curve') # plot it. Can add line plt.plot([0,1], [0,1], 'k:', label='Random Classifier ROC')\n",
    "roc_auc_score(Y_train, Y_train_prediction) # returns the area under the ROC curve, a useful metric\n",
    "\n",
    "# For Linear regression with statistical signifigance:\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "result = sm.ols(formula=\"y ~ x1 + x2 + x3\", data=df).fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hotkeys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ctrl + Shift + Space: toggle param hints\n",
    "- Alt + arrow_key: move selection or down\n",
    "- Ctrl + backspace: delete by words not chatacter\n",
    "- (Ctrl + K), S: save all files\n",
    "- Shift + F12: jump to other uses of a selected variable\n",
    "- Ctrl + page up/page down: navigate between tabs\n",
    "- Ctrl + F2: change the variable name in all areas\n",
    "- Ctrl + K, Ctrl + D: formats the document\n",
    "- Ctr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
